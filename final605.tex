\documentclass[12pt]{article}
\usepackage{fullpage}
\usepackage{amsmath}
\usepackage{amsthm}
\usepackage{amsfonts}
\usepackage{graphicx}
\usepackage{enumitem}
\usepackage{mathtools}
\usepackage{mdframed}
\usepackage[T1]{fontenc}
\usepackage[utf8]{inputenc}
\usepackage[usenames,dvipsnames,svgnames,table]{xcolor}
\definecolor{shadecolor}{gray}{.9}
\usepackage[colorlinks = true,
            linkcolor = blue,
            urlcolor  = blue,
            citecolor = blue,
            anchorcolor = blue]{hyperref}
\usepackage[final]{pdfpages}
\usepackage{pgfplots}
\newtheorem{mydef}{Definition}
\newtheorem{conj}{Conjecture}
\newtheorem{theor}{Theorem}
\pgfplotsset{compat=1.16}

\begin{document}
\begin{center}
\section*{PHYS 605: Final Project}
\subsection*{Circuit Analysis}
\subsubsection*{By Jason Phillips and Izzak Boucher}
\subsubsection*{April 11, 2020}
\end{center}

\tableofcontents
\pagebreak

\section{Introduction}

The goal of this document is to provide a comprehensive analysis of a few interesting and poignant examples in the field of circuitry, without limiting ourselves to the simpler linear case which is easy to handle without the use of more sophisticated techniques and necessarily approximate solutions.  This document will help the writers advance their qualitative understanding of circuit elements in various setups, but will also help them develop the techniques required to do more complicated analysis inherent in their (highly likely) future endeavors in physics research.  So, we attempt to provide analytical solutions to equations when necessary, but a large part of this project will revolve around analyzing the phase space of the equations and doing numerical simulations through Python, Matlab, and LTSpice.  It should also be noted that with the lack of access to actual circuitry we utilize the simulations as stand-ins for actual circuit behavior.  We feel that the analysis we do is reflective of a general method which experimentalists are famiiliar with.  Namely, the use of numerical approximations to solve various first and second order differential equations.  These techniques are also useful in applications which are not circuit related, as many equations of motion are expressible as second order differential equations.  Thus the analogy between this document and the numerical simulations and analysis an experimentalist does on a day to day basis is strong.

\section{The Linear Circuit}

I left some comments which may help (regardless I will add to what you write as I see fit, you can do the same for my stuff IF you'd like).  Here is where I would write any opening remarks about linear equations and the like.

\subsection{First Order: RC circuit}

Homogeneous first-order systems cannot exhibit oscillatory behavior. Look at Strogatz pdf link I sent you on pg 28 (36th page of the pdf).  There can only be oscillatory behavior in the first order case if there is a time dependent current source.  This may not be apparent to you, but the info is in the second chapter of Strogatz.  I can elaborate on this later if you don't.

\subsection{Second Order: The RLC Circuit}

Note that introducing the inductor is what causes the circuit equation to become a second order differential equation, and is hence the reason for the oscillations one sees in the circuit.  Note the comment about homogeneous first-order not being able to oscillate, but in the second order case it is possible.


\section{The Nonlinear Circuit}

In many cases, the linear circuit equations do a great job at explaining what we see experimentally in circuits, but there are many behaviors and interesting phenomena which may only occur in the case where one is analyzing noticeably nonlinear circuitry.  This section will focus on a couple interesting examples, and a few general techniques to utilize when looking at a proposed circuit equation and also looking at how well it matches experiment.

\subsection{Limit Cycles}

One of the most interesting developments within nonlinear dynamics is the concept of a limit cycle.  Limit cycles are closed, isolated trajectories in the phase space of some system of equations which can determine and explain the systems mechanics at large.  These are a strictly nonlinear phenomena.  Note that there are closed trajectories say in the case of the system

\begin{equation}
\begin{bmatrix}
\dot{x} \\ \dot{y}
\end{bmatrix} = 
\begin{bmatrix}
0 & 1 \\
-1 & 0
\end{bmatrix}
\begin{bmatrix}
x \\ y
\end{bmatrix}
\end{equation}

but these closed trajectories are not isolated.  This can be seen from the fact that the trajectories in the phase space of this equation are just a bunch of concentric circles.  A limit cycle can be either stable, unstable, or semi-stable.

\subsection{Josephson Junctions}

Josephson junctions are superconducting devices whose function is based on the studies made by Brian Josephson in 1962, which evidently culminated in him winning the Nobel Prize in 1973.  The "Josephson effect" is a quantum phenomena where when two superconductors are placed within a short distance of each other the "Cooper pairs" in each superconductor can tunnel between the two superconductors, even when there is no voltage difference between them.  This effect can be utilized in order to produce extremely high frequency voltage oscillations, meaning they have many uses as detectors for small voltage differences, amplifiers, and fast switching devices within digital circuits.  These are only a few of their possible applications and for further information the reader is referred to Van Duzer and Turner (1981).


This psuedo-derivation loosely mimics that of the one in Strogatz (1994), but the subsequent analysis is entirely original. The actual analysis of the Josephson junction can be done largely using classical considerations.  We imagine we have a DC current source flowing through our junction, which causes a phase difference in the two superconductors.  This phase difference, we denote by $\phi(t)$, satisfies the so-called Josephson current-phase relationship.

\begin{equation}
I(t) = I_c sin(\phi(t))
\end{equation}

The $I_c$ term is called the critical current, and is maximum current for which there is no voltage difference across the junction.  The voltage difference across the junction can also be expressed in terms of the phase difference, by using the Josephson voltage-phase relation.

\begin{equation}
V = \dfrac{\hbar}{2e} \dot{\phi}
\end{equation}

Now, we turn to an analysis of a common implementation of a Josephson junction by putting it in parallel with a resistor and a capacitor.  Using Kirkhoff's junction law and the fact that there is a $V(t)$ voltage drop across each of these elements we obtain an expression for the variable current source $I(t)$.

\begin{equation}
C \dot{V} +\dfrac{V}{R} + I_c sin(\phi) = I(t)
\end{equation}

Using our earlier relations, this equation simplifies to:  (where $K = \dfrac{2e}{\hbar} $)

\begin{equation}
\ddot{\phi}(t) + \dfrac{1}{RC} \dot{\phi}(t) + \dfrac{K}{C} I_c sin(\phi(t)) = \dfrac{K}{C} I(t)
\end{equation}

This equation should be reminiscent of the driven-damped pendulum, as it is actually identical, if one redefines the variables in an advantageous way.  We also notice that the damping effect-- which initially entered into our equation from the resistor-- becomes less noticeable the more we increase the value of $R$.

Before diving into the specific analysis, a comment is in order, about the translation from our phase difference information into voltage information.  Voltage information is more easily interpretable and measurable, thus we may derive some solutions in terms of the phase difference and use the earlier relationship between the phase and the voltage to determine what the voltage plot will look like.  Firstly, we note that since they are related by a derivative, and we anticipate the behavior of a solution $\phi(t)$ to eventually (and approximately) periodic, that the voltage will also be periodic of the same period, just maybe of a different form.  More importantly, this will suggest that we can infer frequency data for our voltage from our phase data.  This is useful since the Josephson junction's behavior is designed for being able to produce high frequency oscillations.

We now inspect various Josephson Junction behaviors with different forcing currents and various circuit element values.  Note that the numerical value of the constant we determined earlier is $K = \dfrac{2e}{\hbar} = 3 \times 10^{15} Hz/V$ and we will also use $I_c = 1 mA$ since this is a typical critical current in one of these circuits.  We finally note that a typical capacitor being used in one of these circuits is off as small a capacitance as possible in order to increase the frequency at which the Junction can function, thus a typical capacitance of one of these circuits would be in the $pF$ range.  Many future and proposed applications of the Josephson junction involve capacitors in the $fF$ range.

\subsubsection{The Non-damped Case}

In considering our Josephson junction circuit, we anticipate the case where $R = \infty$ and where basically the circuit is just a Josephson Junction and a capacitor.  In this case our previous equation becomes:

\begin{equation}
\ddot{\phi}(t) + \dfrac{K}{C} I_c sin(\phi(t)) = \dfrac{K}{C} I(t)
\end{equation}

Without a driving term-- in circuit problems in general this will be the variable current source $I(t)$-- we obtain:

\begin{equation}
\ddot{\phi}(t) + \dfrac{K}{C} I_c sin(\phi(t)) = 0
\end{equation}

Note that the above equation has a solution in terms of the Jacobi elliptic integral.  But, in general there will be few (if any) closed form solutions to a nonlinear differential equation.  Rather than omit this analysis and resort to the numerical situation we decide to showcase the complementary information mathematical and numerical analysis can yield.  Some things which we could find from the elliptic integral in a fairly accurate manner (like the period of the oscillations) are easily found from the plots themselves in an approximate form.  But, say we want an explicit expression for calculating the value of lots of different points or determine what the phase space trajectories look like.  For this we need to use the method of finding a conserved quantity, ie. a Hamiltonian for the system.  The idea with this functional is that all solution curves will make it stationary.  These curves will be the trajectories of the system.

\begin{equation}
\begin{split}
\dfrac{dH}{dt} & = \partial_{p} H \dfrac{dp}{dt} +
\partial_{q} H \dfrac{dq}{dt} \\ 
& = \dot{q} \dot{p} - \dot{p} \dot{q}  = 0
\end{split}
\end{equation}

Where in the above equation the second line comes from the fact that certain partial derivative relations are necessary ones to make H conserved. These would be:

\begin{equation}
\dot{q} = \partial_{p} H
\end{equation}

\begin{equation}
\dot{p} = -\partial_{q} H
\end{equation}

\begin{equation}
\begin{split}
\dot{\phi}& = \omega \\
& = \partial_{\omega} H(\phi, \omega)
\end{split}
\end{equation}

\begin{equation}
\begin{split}
\dot{\omega}& = - A \sin(\phi) \\
& = -\partial_{\phi} H(\phi, \omega)
\end{split}
\end{equation}

\begin{equation}
\begin{split}
H & =\dfrac{1}{2} \omega^2 +  A \cos(\phi) \\
& = \dfrac{1}{2} \dot{\phi}^2 +  A \cos(\phi)
\end{split}
\end{equation}

We notice that we have essentially integrated the second order differential equation, and the variable $H$ on the right side of the equation is basically the constant of integration.  In a physics situation this variable could be interpreted as the energy, and trajectories with constant energy are the trajectories of a time independent Hamiltonian.  We also note that as a convenience, that during the experiment we will measure some maximum phase difference if our case involves little to no $\dot{\phi}$, but the max $\dot{\phi}$ and $\phi$ if we are looking for the solutions with large initial velocities.

\begin{equation}
H_0 = A cos(\phi_{0})
\end{equation}

\begin{equation}
H_0 \dfrac{1}{2}\omega_0^2 + Acos(\phi_0)
\end{equation}

We can then rearrange the last line of the equation in order to find another differential equation.

\begin{equation}
\dfrac{d\phi}{dt} = \pm \sqrt{2E -  2A \cos(\phi)}
\end{equation}

This differential equation is not reducible by any known methods, so we decide to look at an important quantity, the period.
1
\begin{equation}
\dfrac{dt}{d\phi} = \dfrac{1}{\pm \sqrt{2H_0 -  2A \cos(\phi)}}
\end{equation}

Using some unit analysis we find the value of the parameter in the above equation is basically:

\begin{equation}
\dfrac{KI_c}{C} = \dfrac{3 \times 10^{21} Hz}{sec} \dfrac{1}{C_n}
\end{equation}

Where the variable $C_n$ is the numerical value of the capacitor in $pF$.

The following phase difference plots all start at the same initial (maximum possible) phase difference of $\pi / 2$.

\begin{figure}
\caption{Phase Difference of Nondriven-undamped Josephson Junction}
\begin{center}
\includegraphics[scale=0.50]{uud-jjsmallc.png}
\end{center}
\end{figure}

\begin{figure}
\caption{Phase Difference of Nondriven-undamped Josephson Junction}
\begin{center}
\includegraphics[scale=0.50]{uud-jjlargec.png}
\end{center}
\end{figure}


We note that the behavior of the circuit is approximately sinusoidal for smaller initial phase differences, but for cases where the initial phase difference is larger, the more rounded curve which one associates with the elliptic integral is visible.  It should also be noted that the frequency of the oscillations decreases when a larger initial phase is chosen.  Thus, a large phase difference actually excludes us from detecting smaller frequencies.  So, experimentally it is not advantageous to utilize a super large starting phase difference in the superconductors.

\begin{figure}
\caption{Phase Difference of Nondriven-undamped Josephson Junction}
\begin{center}
\includegraphics[scale=0.50]{uud-jjinit.png}
\end{center}
\end{figure}

\begin{figure}
\caption{Voltage Difference of Nondriven-undamped Josephson Junction}
\begin{center}
\includegraphics[scale=0.50]{uud-jjinitv.png}
\end{center}
\end{figure}


We now decide to inspect our system while it is under a constant driving current $I_b=1mA$, which is realistic and reminds one of the typical situation in which the Josephson Junction is implemented.  The curve we see is exactly what experiment shows, namely that there is some delay in the voltage over the capacitor, but shortly there is a small jump in voltage which turns into a linear function of time.  This shows the manner in which the undamped (no resistor version) of the Josephson Junction behaves.

\begin{equation}
\ddot{\phi}(t) + \dfrac{K}{C} I_c sin(\phi(t)) = \dfrac{K}{C} I_b
\end{equation}

\begin{figure}
\caption{Voltage Difference of Driven-undamped Josephson Junction}
\begin{center}
\includegraphics[scale=0.50]{ud-jjinitv.png}
\end{center}
\end{figure}

\subsubsection{The Damped Case}

In this case we do not consider the presence of the resistor negligible, thus we use the governing equation

\begin{equation}
\ddot{\phi}(t) + \dfrac{1}{RC} \dot{\phi}(t) + \dfrac{K}{C} I_c sin(\phi(t)) = \dfrac{K}{C} I(t)
\end{equation}

We notice that because of the scale of the problem that the value of the resistor only seems to have some apparent effect as we begin to drop below the $1 n\Omega$ mark.  Otherwise, the damping is not totally perceivable.  In the sense that our unit needs to remain at a pretty much constant high frequency for only a $0.1 ns$ or so for us to have reliable high frequency sensitivity.

\begin{figure}
\caption{Phase Difference of Nondriven-damped Josephson Junction}
\begin{center}
\includegraphics[scale=0.50]{dud-jjres.png}
\end{center}
\end{figure}

\begin{figure}
\caption{Voltage Difference of Nondriven-damped Josephson Junction}
\begin{center}
\includegraphics[scale=0.50]{dud-jjresv.png}
\end{center}
\end{figure}

Next we introduce a $1 mA$ driving current into the system and inspect how the different resistors make the corresponding phase differences and voltage differences change over time.  I noticed something very interesting which depended on the initial phase difference, which is that if the two superconducting materials started at a phase difference of $\pi / 2$ that there was relatively no change in the phase difference as time went on, where if one started with a low phase difference that the resulting solution increased incredibly fast.  Not only this, but the larger the damping force (ie. the smaller the resistance of the resistor) the less likely the solution was from becoming non zero.

\begin{figure}
\caption{Voltage Difference of Driven-damped J.J. $\Delta \phi_0 = \pi / 2$}
\begin{center}
\includegraphics[scale=0.50]{dd-jjres.png}
\end{center}
\end{figure}

\begin{figure}
\caption{Voltage Difference of Driven-damped J.J. $\Delta \phi_0 = \pi / 2$}
\begin{center}
\includegraphics[scale=0.50]{dd-jjresv.png}
\end{center}
\end{figure}



\begin{figure}
\caption{Voltage Difference of Driven-damped J.J. $\Delta \phi_0 = 0$}
\begin{center}
\includegraphics[scale=0.50]{dd-jjresphase0.png}
\end{center}
\end{figure}

\begin{figure}
\caption{Voltage Difference of Driven-damped J.J. $\Delta \phi_0 = 0$}
\begin{center}
\includegraphics[scale=0.50]{dd-jjresphase0v.png}
\end{center}
\end{figure}


\subsection{van der Pol Oscillator}

\subsubsection{Background}

Balthasar van der Pol was a physicist working at Philips Research Laboratories.  He was an important contributor to the field of nonlinear dynamics, specifically in relation to the concept of a nonlinear oscillator.  He helped advance the quantitative development of the radio and the following equation we will decide to analyze is the van der Pol equation which pertains to the circuitry within these early radios.  He and Josephson were some of the early pioneers in the subject and a lot of concepts, specifically that of a limit cycle are largely due to the work that van der Pol did.  He also coined the idea of a relaxation oscillation, which is a strictly nonlinear phenomena, where the oscillation quickly returns to to equilibrium after a small period of relatively slow movement.  This concept is prominent in many applications: instrument strings, neurons firing, cell function.



\subsubsection{Weakly Nonlinear Oscillators}
A weakly nonlinear oscillator is the solution to a differential equation of the form: (Where $0<\epsilon << 1$)

\begin{equation}
\ddot{x} + x + \epsilon h(x, \dot{x}) = 0
\end{equation}

These equations represent a small perturbation of the linear equation. 

\begin{equation}
\ddot{x} + x = 0
\end{equation}

We decide to focus on the van der Pol oscillator, which is a good equation for describing the voltage difference within a tetrode multivibrator circuit.  These circuits had uses in early commercial radios and were the basis for a lot of his work on nonlinear oscillations. 

\begin{equation}
\ddot{x} + x + \epsilon (x^2 -1)\dot{x} = 0
\end{equation}

Why is regular perturbation theory not the best method to analyze these equations?  Well, for larger values of $\epsilon$ the number of the terms required for the perturbative analysis is more than just one or two.  We see as a result that the solutions we can obtain via this method are extremely inaccurate for large time scales.  The reader is reminded that the regular perturbation theory suggests a solution decomposition of the form:

\begin{equation}
x(t) = x_0(t) + \epsilon x_1(t) + \epsilon^2 x_2(t) + ... = \sum_{i=0}^{\infty} \epsilon^i x_i(t)
\end{equation}

The insight that one needs in order to properly asses these problems is to realize that the oscillatory solution may be dictated by multiple time scales.  A good example of this is the following equation.

\begin{equation}
\ddot{x} + 2 \epsilon \dot{x}+ x  = 0
\end{equation}

Whose solution has two components: an oscillatory solution, and an exponential decay.  Both these behaviors are not encapsulated by regular perturbation theory, which after only a two term expansion is a vastly inaccurate answer for large time scales.  

We denote $\tau = t$ as the fast $O(1)$ time and $T = \epsilon t$ as the slower time scale.  Thus, we propose a solution of the form below.


\begin{equation}
x(t, \epsilon) = x_0(\tau, T) + \epsilon x_1(\tau, T) + O(\epsilon^2) = \sum_{i=0}^{\infty} \epsilon^i x_i(\tau, T)
\end{equation}

We proceed by substituting this expression into the weak nonlinear oscillator equation.

\begin{equation}
\ddot{x} + x + \epsilon h(x, \dot{x}) = 0
\end{equation}

We then separate this expression into different powers of $\epsilon$ and making sure each component is 0 and we obtain (for the $O(1)$ and $O(\epsilon)$ terms) the two-timing equations:

\begin{equation}
\partial_{\tau \tau} x_0  + x_0 = 0
\end{equation}

\begin{equation}
\partial_{\tau \tau} x_1 + x_1 = -2\partial_{T \tau} x_0 -h
\end{equation}


Notice that we could continue looking at higher order terms, but they get more complicated.  And, the added precision of the $O(\epsilon^2)$ may not be necessary or easy.

\begin{equation}
x_0(t) = R(T) cos(\tau + \phi(T))
\end{equation}

\begin{equation}
\partial_{\tau \tau} x_1 + x_1  = 2(R'(T)sin(\tau + \phi(T)) + R(T)cos(\tau + \phi(T)) -h
\end{equation}

We do not want any forcing terms within our $O(\epsilon)$ equation so we set the R.H.S equal to 0 and obtain an expression for $h$.

\begin{equation}
h(\phi) = 2(R'sin(\tau + \phi) + R \phi' cos(\tau + \phi))
\end{equation}

As the equation for $h$ is constant with respect to $\tau$ we make the substitution:

\begin{equation}
\theta = \tau + \phi
\end{equation}

\begin{equation}
h(\theta) = 2(R'sin(\theta) + R \phi' cos(\theta))
\end{equation}

By performing a Fourier decomposition we obtain two differential equations in $T$.

\begin{equation}
R' = \dfrac{1}{2 \pi} \int_{-\pi}^{\pi} h(\theta) sin(\theta) d \theta = \dfrac{1}{2} \langle h, sin(\theta) \rangle
\end{equation}

\begin{equation}
R \phi' = \dfrac{1}{2 \pi} \int_{-\pi}^{\pi} h(\theta) cos(\theta) d \theta  = \dfrac{1}{2}\langle h, cos(\theta) \rangle
\end{equation}

Not losing sight of our initial problem, we turn to analyzing the slightly nonlinear van der Pol equation, now that we have developed the machinery to do so.

\begin{equation}
h = (x^2 - 1) \dot{x} = (r^2 \cos^2(\theta) -1)(-r \sin(\theta)) 
\end{equation}

\begin{equation}
r' = - \dfrac{1}{2} \langle r^3 \cos^2(\theta) \sin(\theta), \sin(\theta) \rangle +  \dfrac{1}{2} \langle r \sin(\theta), \sin(\theta) \rangle
\end{equation}

\begin{equation}
r' = -\dfrac{1}{8} r^3 + \dfrac{1}{2}r
\end{equation}

\begin{equation}
r \phi' = - \dfrac{1}{2} \langle r^3 \cos^2(\theta) \sin(\theta), \cos(\theta) \rangle +  \dfrac{1}{2} \langle r \sin(\theta), \sin(\theta) \rangle = 0
\end{equation}

Through a partial fraction decomposition we can solve the equation for $r'$.

\begin{equation}
\begin{split}
 -dT & = 8 \dfrac{dr}{r(r^2-4)} \\
 & = 8 \dfrac{dr}{r(r-2)(r+2)} \\
 & = 8( -\dfrac{dr}{4r}  + \dfrac{dr}{8(r-2)}+\dfrac{dr}{8(r+2)}) \\
 & = -\dfrac{2dr}{r}  + \dfrac{dr}{(r-2)}+\dfrac{dr}{(r+2)}
\end{split}
\end{equation}

\begin{equation}
-T = C + \ln \dfrac{(r^2-4)}{r^2} 
\end{equation}

\begin{equation}
Ae^{-T} = 1 + \dfrac{-4}{r^2}
\end{equation}

\begin{equation}
r(T)=\dfrac{2}{\sqrt{1 + Ae^{-T}}}
\end{equation}

Taking the initial displacement of the oscillator to be $r_0$ at $t = 0$ we find that

\begin{equation}
\dfrac{4}{r_0^2} - 1 = A
\end{equation}

\begin{equation}
\phi' = 0 \implies \phi = \delta
\end{equation}

Thus, only needing the first term in our two-timing perturbative solution we find that our approximate solution to the slightly nonlinear van der Pol equation is:

\begin{equation}
x(t) = \dfrac{2}{\sqrt{1 + (4/r_0^2 - 1)e^{-\epsilon t}}} \cos(t + \delta)
\end{equation}

Notice that the concept of a limit cycle is obvious in this equation as the time $t$ gets pretty large the displacement during each approaches 2 units from equilibrium.

\subsection{Limit Cycles in van der Pol Equation}

\begin{equation}

\end{equation}


\subsection{Duffing Oscillator}

In the case where one is analyzing circuitry in which there is a sense of the restoring force being nonlinear we have the first order approximation of the Josephson Junction in the form of the Duffing equation.

\begin{equation}
\ddot{x} + x + \epsilon x^3 = 0
\end{equation}

As one can see, if we took a Taylor expansion of the $\sin$ function in the Josephson junction equation we would have obtained the Duffing equation.  Thus, even though we cannot find an explicit expression for the actual Josephson junction phase equation, we can find a perturbative solution to the Duffing equation and adjust it to fit the Josephson Junction model as needed.  In any case, we see that the Duffing equation could be thought of as modeling a spring which becomes more difficult or less difficult to stretch as the displacement becomes greater.  This property depends on the sign of the $\epsilon$ term.


\end{document}



	